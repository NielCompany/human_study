# # gemini_utils.py
# import os
# from dotenv import load_dotenv
# import google.generativeai as genai
# from PIL import Image

# # .env íŒŒì¼ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
# load_dotenv()

# # Gemini API ì„¤ì •
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# if not GEMINI_API_KEY:
#     raise ValueError("GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# genai.configure(api_key=GEMINI_API_KEY)


# # ì²« ì¸ì‚¬ í”Œë˜ê·¸ & ëŒ€í™” ì €ì¥
# greeted = False
# conversation_history = []

# # Gemini ëª¨ë¸ ì„ íƒ (flash ë˜ëŠ” pro ì‚¬ìš© ê°€ëŠ¥)
# model = genai.GenerativeModel("gemini-1.5-flash")

# def explain_by_gemini(prompt: str) -> str:
#     """í•˜ì ì˜ˆì¸¡ ê²°ê³¼ì™€ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gemini ì„¤ëª… ìƒì„±"""
#     response = model.generate_content(prompt)
#     # response = model.generate_content(question) 
#     return response.text.strip()


import os
from dotenv import load_dotenv
import google.generativeai as genai

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# 2. ëª¨ë¸ ì´ˆê¸°í™”
llm_model = genai.GenerativeModel("gemini-2.0-flash")

# # 3. í”„ë¡œí•„ í…ìŠ¤íŠ¸ ì „ì²´ ë¡œë“œ
# PROFILE_PATH = os.path.normpath(os.path.join(os.path.dirname(__file__), "../../data-files/bot_profile.txt"))
# with open(PROFILE_PATH, "r", encoding="utf-8") as f:
#     profile_text = f.read()

# 4. ì²« ì¸ì‚¬ í”Œë˜ê·¸ & ëŒ€í™” ì €ì¥
greeted = False
conversation_history = []

# 5. ë©”ì¸ í•¨ìˆ˜
def ask_gemini(question):
    global greeted, conversation_history

    greeting = "" if greeted else "ì•ˆë…•í•˜ì„¸ìš”. \nê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!\n"
    greeted = True

    # ëŒ€í™” ê¸°ë¡ í…ìŠ¤íŠ¸ ìƒì„±
    history_text = ""
    for i, (q, a) in enumerate(conversation_history[-6:]):
        history_text += f"[ëŒ€í™”{i+1}]\nì‚¬ìš©ì: {q}\nAI: {a}\n"

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
            ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ í•˜ì ì ê²€ ì „ë¬¸ê°€ AI ì±—ë´‡ì…ë‹ˆë‹¤.

            ì´ì „ ëŒ€í™” ê¸°ë¡:
            {history_text}

            ì§€ì¹¨:
            - ì¸ì‚¿ë§ì€ ì±—ë´‡ì´ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ì²« ì¸ì‚¬ëŠ” ì‹œìŠ¤í…œì´ ì´ë¯¸ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.
            - ë³¸ë¡ ë§Œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
            - ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ ì¼ë°˜ì ì¸ AI ì§€ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
            - ë™ì¼ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ì¤„ë°”ê¿ˆê³¼ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
            - ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í‘œí˜„ì„ ë‹¤ì–‘í•˜ê²Œ ë°”ê¿”ì„œ ì„¤ëª…í•˜ì„¸ìš”.
            - ëŠ¥ìˆ™í•˜ë‹¤ëŠ” í‘œí˜„ ëŒ€ì‹  ì–´ë–¤ í™œë™ì—ì„œ ì‚¬ìš©í–ˆëŠ”ì§€ë„ ì–¸ê¸‰í•˜ì„¸ìš”.
            ì‚¬ìš©ì ì§ˆë¬¸: {question}
            """
    print("ğŸ§ª ì‚¬ìš©ì ì§ˆë¬¸:", question)

    try:
        response = llm_model.generate_content([prompt])
        answer = response.text.strip()

        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        conversation_history.append((question, answer))

        return greeting + answer
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"