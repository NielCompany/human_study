# ðŸ“ app/utils/rag_utils.py

import os
import pickle
import re
import warnings
import threading
from dotenv import load_dotenv
from .gemini_utils import ask_gemini
import google.generativeai as genai

# suppress TensorFlow and other logs
os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0')
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# 1. ENV ë¡œë“œ & Gemini API í‚¤ ì„¤ì •
load_dotenv()

genai.configure(api_key=os.getenv('GEMINI_API_KEY'))

# 2. ì „ì—­ ìºì‹œ ë³€ìˆ˜ ì„ ì–¸
VECTORSTORE_PATH = os.path.join(
    os.path.dirname(__file__),
    'faiss_db',
    'vectorstore.pkl'
)
_vectorstore = None
_llm = None
conversation_history = []

# 3. Vectorstore ë¹„ë™ê¸° ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ)
def _load_vectorstore_background():
    global _vectorstore
    try:
        with open(VECTORSTORE_PATH, 'rb') as f:
            _vectorstore = pickle.load(f)
    except Exception:
        _vectorstore = None

threading.Thread(
    target=_load_vectorstore_background,
    daemon=True
).start()

# 4. Lazy ë°˜í™˜ í•¨ìˆ˜
def get_vectorstore():
    # ë°±ê·¸ë¼ìš´ë“œ ë¡œë“œê°€ ì™„ë£Œëœ _vectorstore ê°ì²´ë¥¼ ë°˜í™˜
    return _vectorstore

def get_llm():
    global _llm
    if _llm is None:
        try:
            # ì´ë¯¸ configureëœ genaië¥¼ ìž¬ì‚¬ìš©
            _llm = genai.GenerativeModel('gemini-2.0-flash')
        except Exception:
            _llm = None
    return _llm

# 5. ì‘ë‹µ ì •ë¦¬ í•¨ìˆ˜ (unchanged)
def format_answer(answer: str) -> str:
    answer = re.sub(r"(ì•ˆë…•í•˜ì„¸ìš”[.!]?\s*ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!?)", "", answer).strip()
    answer = re.sub(r"\*\*(.*?)\*\*", r"\1", answer)
    answer = re.sub(r"(\d+)\.(\S)", r"\1. \2", answer)
    answer = re.sub(r"\n?[\-\*]{1,2} ?([^\n]+)", r"\n  - \1", answer)
    answer = re.sub(r"\n{3,}", "\n\n", answer)
    answer = re.sub(r"\n[ \t]+", "\n", answer)
    answer = re.sub(r"^AI:\s*", "", answer)
    return answer.strip()

# 6. ë©”ì¸ RAG í•¨ìˆ˜ (prompt ë¡œì§ ìˆ˜ì • ì—†ì´ ì§€ì—° ë¡œë“œë§Œ ì ìš©)
def answer_with_rag(query: str, top_k: int = 3) -> str:
    global conversation_history

    vectorstore = get_vectorstore()
    print("vectorstore loaded:", bool(vectorstore))
    if not vectorstore:
        return ask_gemini(query)

    docs = vectorstore.similarity_search(query, k=top_k)
    if not docs or all(len(doc.page_content.strip()) < 30 for doc in docs):
        return ask_gemini(query)

    context = '\n\n'.join(doc.page_content for doc in docs)
    history_text = ''.join(
        f"[ëŒ€í™”{i+1}]\nì‚¬ìš©ìž: {q}\nAI: {a}\n"
        for i, (q, a) in enumerate(conversation_history[-6:])
    )

    prompt = f"""
    ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ í•˜ìž ì ê²€ AI ë¹„ì„œìž…ë‹ˆë‹¤.
    ì•„ëž˜ ë¬¸ì„œì™€ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìž ì§ˆë¬¸ì— ê·¼ê±° ìžˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

    [ë¬¸ì„œ ìš”ì•½]
    {context}

    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {history_text}

    [ì§ˆë¬¸]
    {query}

    [ì§€ì¹¨]
    - ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ì„œìˆ 
    - ì¤„ë°”ê¿ˆê³¼ ë“¤ì—¬ì“°ê¸°ë¡œ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
    - ë¬¸ì„œë‚˜ ì‹œìŠ¤í…œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„, ìœ ì‚¬í•œ ì‚¬ë¡€ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì— ê¸°ë°˜í•´ ì •ì¤‘í•˜ê³  ìœ ìš©í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    - "í•´ë‹¹ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ë˜ëŠ” "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", í˜„ìž¬ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™€ ê°™ì€ ë¶€ì •ì ì¸ í‘œí˜„ì€ í”¼í•˜ì„¸ìš”.
    - ì¸ì‚¿ë§ì€ ì±—ë´‡ì´ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ì²« ì¸ì‚¬ëŠ” ì‹œìŠ¤í…œì´ ì´ë¯¸ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.
    - ë™ì¼ ë¬¸ìž¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ì¤„ë°”ê¿ˆê³¼ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    - ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í‘œí˜„ì„ ë‹¤ì–‘í•˜ê²Œ ë°”ê¿”ì„œ ì„¤ëª…í•˜ì„¸ìš”.
    - ëŠ¥ìˆ™í•˜ë‹¤ëŠ” í‘œí˜„ ëŒ€ì‹  ì–´ë–¤ í™œë™ì—ì„œ ì‚¬ìš©í–ˆëŠ”ì§€ë„ ì–¸ê¸‰í•˜ì„¸ìš”.
    - í•­ëª© ë‚˜ì—´ ì‹œ ìˆ«ìžë‚˜ í•˜ì´í”ˆ(-), ë“¤ì—¬ì“°ê¸°, ì¤„ë°”ê¿ˆì„ ëª…í™•í•˜ê²Œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
    - ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹  ì¼ë°˜ í…ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ì„¸ìš”. ì˜ˆ: "1. í•­ëª©", " - ì„¸ë¶€ì‚¬í•­", ë¹ˆ ì¤„ ì‚¬ìš©
    """

    try:
        llm = get_llm()
        response = llm.generate_content(prompt)
        answer = format_answer(response.text.strip())
        conversation_history.append((query, answer))
        return answer
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"
