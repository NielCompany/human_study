# ğŸ“ app/utils/rag_utils.py
import os
import pickle
import re
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import google.generativeai as genai
from .gemini_utils import ask_gemini



# 1. í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ì„¤ì •
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
llm = genai.GenerativeModel("gemini-2.0-flash")

# 2. ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
VECTORSTORE_PATH = os.path.join(os.path.dirname(__file__), "faiss_db", "vectorstore.pkl")

# 3. ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")

# 4. ëŒ€í™” ê¸°ë¡ ì „ì—­ ë³€ìˆ˜
greeted = False
conversation_history = []

# 5. ì‘ë‹µ ì •ë¦¬ í•¨ìˆ˜
def format_answer(answer: str) -> str:
    answer = re.sub(r"(ì•ˆë…•í•˜ì„¸ìš”[.!]?\s*ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!?)*", "", answer).strip()
    answer = re.sub(r"\*\*(.*?)\*\*", r"\1", answer)
    answer = re.sub(r"(\d+)\.(\S)", r"\1. \2", answer)
    answer = re.sub(r"\n?[\-\*]{1,2} ?([^\n]+)", r"\n  - \1", answer)
    answer = re.sub(r"\n{3,}", "\n\n", answer)
    answer = re.sub(r"\n[ \t]+", "\n", answer)
    answer = re.sub(r"^AI:\s*", "", answer)
    return answer.strip()

# âœ… 6. ë©”ì¸ RAG í•¨ìˆ˜
def answer_with_rag(query: str, top_k=3) -> str:
    global conversation_history

    # 1. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    if not os.path.exists(VECTORSTORE_PATH):
        return ask_gemini(query)  # fallback to ì¼ë°˜ ì§€ì‹ ëª¨ë“œ

    with open(VECTORSTORE_PATH, "rb") as f:
        vectorstore = pickle.load(f)

    # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    docs: list[Document] = vectorstore.similarity_search(query, k=top_k)

    # 2-1. ê´€ë ¨ ë¬¸ì„œê°€ ê±°ì˜ ì—†ìœ¼ë©´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ fallback
    if not docs or all(len(doc.page_content.strip()) < 30 for doc in docs):
        return ask_gemini(query)  # âœ… ì¼ë°˜ LLM ë‹µë³€ìœ¼ë¡œ fallback

    # 3. contextì™€ ëŒ€í™” ê¸°ë¡ êµ¬ì„±
    context = "\n\n".join([doc.page_content for doc in docs])
    history_text = ""
    for i, (q, a) in enumerate(conversation_history[-6:]):
        history_text += f"[ëŒ€í™”{i+1}]\nì‚¬ìš©ì: {q}\nAI: {a}\n"

    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ í•˜ì ì ê²€ AI ë¹„ì„œì…ë‹ˆë‹¤.
    ì•„ë˜ ë¬¸ì„œì™€ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ê·¼ê±° ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

    [ë¬¸ì„œ ìš”ì•½]
    {context}

    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {history_text}

    [ì§ˆë¬¸]
    {query}

    [ì§€ì¹¨]
    - ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ì„œìˆ 
    - ì¤„ë°”ê¿ˆê³¼ ë“¤ì—¬ì“°ê¸°ë¡œ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
    - ë¬¸ì„œë‚˜ ì‹œìŠ¤í…œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„, ìœ ì‚¬í•œ ì‚¬ë¡€ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì— ê¸°ë°˜í•´ ì •ì¤‘í•˜ê³  ìœ ìš©í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    - "í•´ë‹¹ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ë˜ëŠ” "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", í˜„ì¬ ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™€ ê°™ì€ ë¶€ì •ì ì¸ í‘œí˜„ì€ í”¼í•˜ì„¸ìš”.
    - ì¸ì‚¿ë§ì€ ì±—ë´‡ì´ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ì²« ì¸ì‚¬ëŠ” ì‹œìŠ¤í…œì´ ì´ë¯¸ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.
    - ë™ì¼ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ì¤„ë°”ê¿ˆê³¼ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    - ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í‘œí˜„ì„ ë‹¤ì–‘í•˜ê²Œ ë°”ê¿”ì„œ ì„¤ëª…í•˜ì„¸ìš”.
    - ëŠ¥ìˆ™í•˜ë‹¤ëŠ” í‘œí˜„ ëŒ€ì‹  ì–´ë–¤ í™œë™ì—ì„œ ì‚¬ìš©í–ˆëŠ”ì§€ë„ ì–¸ê¸‰í•˜ì„¸ìš”.
    - í•­ëª© ë‚˜ì—´ ì‹œ ìˆ«ìë‚˜ í•˜ì´í”ˆ(-), ë“¤ì—¬ì“°ê¸°, ì¤„ë°”ê¿ˆì„ ëª…í™•í•˜ê²Œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
    - ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹  ì¼ë°˜ í…ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ì„¸ìš”. ì˜ˆ: "1. í•­ëª©", " - ì„¸ë¶€ì‚¬í•­", ë¹ˆ ì¤„ ì‚¬ìš©
    """

    try:
        response = llm.generate_content(prompt)
        answer = format_answer(response.text.strip())
        conversation_history.append((query, answer))
        return answer
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


